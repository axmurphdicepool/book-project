{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc0778c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## called PRH API again to find out the categories called 'classics'\n",
    "## exported classics based on these categories\n",
    "## then worked with CGPT to 1) clean these results (.json to df, keep only relevant columns) and 2) enrich and deduplicate against existing book list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dffde18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_json('Categories-1765200986129.json')\n",
    "df2 = pd.read_json('Categories-1765201667362.json')\n",
    "df3 = pd.read_json('Categories-1765201975361.json')\n",
    "df4 = pd.read_json('Categories-1765202008959.json')\n",
    "df5 = pd.read_json('Categories-1765202091477.json')\n",
    "df6 = pd.read_json('Categories-1765202115878.json')\n",
    "df7 = pd.read_json('Categories-1765202145247.json')\n",
    "df8 = pd.read_json('Categories-1765202167643.json')\n",
    "df9 = pd.read_json('Categories-1765202243894.json')\n",
    "df10 = pd.read_json('Categories-1765202265960.json')\n",
    "df11 = pd.read_json('Categories-1765202289743.json')\n",
    "df12 = pd.read_json('Categories-1765202313896.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f46df6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean1 = pd.DataFrame(df1.data.titles)\n",
    "df_clean2 = pd.DataFrame(df2.data.titles)\n",
    "df_clean3 = pd.DataFrame(df3.data.titles)\n",
    "df_clean4 = pd.DataFrame(df4.data.titles)\n",
    "df_clean5 = pd.DataFrame(df5.data.titles)\n",
    "df_clean6 = pd.DataFrame(df6.data.titles)\n",
    "df_clean7 = pd.DataFrame(df7.data.titles)\n",
    "df_clean8 = pd.DataFrame(df8.data.titles)\n",
    "df_clean9 = pd.DataFrame(df9.data.titles)\n",
    "df_clean10 = pd.DataFrame(df10.data.titles)\n",
    "df_clean11 = pd.DataFrame(df11.data.titles)\n",
    "df_clean12 = pd.DataFrame(df12.data.titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64741325",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated = pd.DataFrame(pd.concat([df_clean1, df_clean2, df_clean3, df_clean4, df_clean5, df_clean6, df_clean7, df_clean8, df_clean9, df_clean10, df_clean11, df_clean12], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1eab3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = ['title', 'author', 'price', 'pages', ]\n",
    "df_new = concatenated[cols_to_keep]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98380555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df_new['length'] = np.where(df_new['pages'] > 350, 'long',\n",
    "                np.where(df_new['pages'] < 250, 'short', 'average'))\n",
    "\n",
    "df_new['classics'] = 1\n",
    "df_new['classics'] = df_new['classics'].astype('int64')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde7a0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_expanded = df_new['price'].apply(lambda x: x[0]).apply(pd.Series)\n",
    "df = pd.concat([df_new, price_expanded], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fffba9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rates = {\n",
    "    'USD': 1/1.165,  # ~0.859\n",
    "    'CAD': 1/1.63,   # ~0.613\n",
    "    'EUR': 1.0       # if some prices already in EUR\n",
    "}\n",
    "\n",
    "def price_to_eur(price_entry):\n",
    "    d = price_entry[0]  # assuming list with a dict\n",
    "    amount = d.get('amount')\n",
    "    curr = d.get('currencyCode')\n",
    "    rate = rates.get(curr)\n",
    "    return amount * rate if amount is not None and rate is not None else None\n",
    "\n",
    "df['price_eur'] = df_new['price'].apply(price_to_eur)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5851fc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = ['title', 'author', 'length', 'classics','price_eur']\n",
    "df_final = df[cols_to_keep]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4e11f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_existing = pd.read_csv('deduplicated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f81e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rapidfuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e065af10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rapidfuzz import fuzz, process\n",
    "import re\n",
    "\n",
    "# Make copies\n",
    "df_existing = df_existing.copy()\n",
    "df_final = df_final.copy()\n",
    "\n",
    "# Ensure 'classics' exists\n",
    "if 'classics' not in df_existing.columns:\n",
    "    df_existing['classics'] = 0\n",
    "\n",
    "# Make sure all rows in df_final have classics = 1\n",
    "df_final['classics'] = 1\n",
    "\n",
    "# Normalization function (lowercase, strip punctuation, normalize spaces)\n",
    "def normalize_text(s):\n",
    "    s = str(s).lower()\n",
    "    s = re.sub(r'\\s+', ' ', s)\n",
    "    s = re.sub(r'[^\\w\\s]', '', s)\n",
    "    return s.strip()\n",
    "\n",
    "# Normalize columns\n",
    "df_existing['author_norm'] = df_existing['author'].apply(normalize_text)\n",
    "df_existing['title_norm'] = df_existing['title'].apply(normalize_text)\n",
    "df_final['author_norm'] = df_final['author'].apply(normalize_text)\n",
    "df_final['title_norm'] = df_final['title'].apply(normalize_text)\n",
    "\n",
    "# Blocking key: first letter of author + first letter of title\n",
    "df_existing['block_key'] = df_existing['author_norm'].str[0] + df_existing['title_norm'].str[0]\n",
    "df_final['block_key'] = df_final['author_norm'].str[0] + df_final['title_norm'].str[0]\n",
    "\n",
    "matched_indices_final = []\n",
    "threshold = 70  # similarity threshold\n",
    "\n",
    "# Fuzzy matching within blocks\n",
    "for block in df_final['block_key'].unique():\n",
    "    block_final = df_final[df_final['block_key'] == block]\n",
    "    block_existing = df_existing[df_existing['block_key'] == block]\n",
    "\n",
    "    if block_existing.empty:\n",
    "        continue\n",
    "\n",
    "    existing_strings = (block_existing['author_norm'] + ' ' + block_existing['title_norm']).tolist()\n",
    "\n",
    "    for _, row in block_final.iterrows():\n",
    "        query = f\"{row['author_norm']} {row['title_norm']}\"\n",
    "        result = process.extractOne(query, existing_strings, scorer=fuzz.token_set_ratio, score_cutoff=threshold)\n",
    "        if result is not None:\n",
    "            match, score, idx = result\n",
    "            existing_idx = block_existing.index[idx]\n",
    "\n",
    "            # Average price_eur\n",
    "            df_existing.at[existing_idx, 'price_eur'] = np.nanmean([\n",
    "                df_existing.at[existing_idx, 'price_eur'],\n",
    "                row['price_eur']\n",
    "            ])\n",
    "            # Set classics for matched row\n",
    "            df_existing.at[existing_idx, 'classics'] = 1\n",
    "            matched_indices_final.append(row.name)  # <-- fixed index here\n",
    "\n",
    "# Append unmatched rows from df_final\n",
    "df_to_append = df_final.drop(index=matched_indices_final)\n",
    "\n",
    "# Ensure all appended rows have classics = 1 (redundant but safe)\n",
    "df_to_append['classics'] = 1\n",
    "\n",
    "# Add missing columns with NaN\n",
    "for col in df_existing.columns:\n",
    "    if col not in df_to_append.columns:\n",
    "        df_to_append[col] = np.nan\n",
    "\n",
    "# Reorder columns to match df_existing\n",
    "df_to_append = df_to_append[df_existing.columns]\n",
    "\n",
    "# Combine\n",
    "df_merged = pd.concat([df_existing, df_to_append], ignore_index=True)\n",
    "\n",
    "# Drop temporary columns\n",
    "df_merged = df_merged.drop(columns=['author_norm', 'title_norm', 'block_key'])\n",
    "\n",
    "# Optional: check counts\n",
    "print(f\"Number of 1s in df_existing: {df_existing['classics'].sum()}\")\n",
    "print(f\"Number of 1s in df_merged: {df_merged['classics'].sum()}\")\n",
    "print(f\"Number of df_final rows appended: {len(df_to_append)}\")\n",
    "print(f\"Number of df_final rows matched: {len(matched_indices_final)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33d278f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.to_csv('added_classics.csv')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
